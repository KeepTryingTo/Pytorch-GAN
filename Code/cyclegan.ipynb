{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nflag = 0\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        flag = 1\n        break\n    if flag == 1:\n        break\n    \n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-04-29T12:13:30.673195Z","iopub.execute_input":"2023-04-29T12:13:30.673608Z","iopub.status.idle":"2023-04-29T12:13:30.687190Z","shell.execute_reply.started":"2023-04-29T12:13:30.673566Z","shell.execute_reply":"2023-04-29T12:13:30.686079Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"/kaggle/input/cyclegandataset/vangogh2photo/val/testB/2014-08-16 00_41_44.jpg\n","output_type":"stream"}]},{"cell_type":"code","source":"\"\"\"\n@Author : Keep_Trying_Go\n@Major  : Computer Science and Technology\n@Hobby  : Computer Vision\n@Time   : 2023/4/28 17:14\n\"\"\"\n\nimport torch\nimport torchvision\nfrom torchvision import transforms\n\nclass Block(torch.nn.Module):\n    def __init__(self,in_channels,out_channels,stride):\n        super(Block, self).__init__()\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n\n        self.conv = torch.nn.Sequential(\n            torch.nn.Conv2d(in_channels=in_channels,out_channels=out_channels,kernel_size=(4,4),\n                            stride=stride,padding=1,bias=True,padding_mode='reflect'),\n            torch.nn.BatchNorm2d(num_features=out_channels),\n            torch.nn.LeakyReLU(negative_slope=0.2,inplace=True)\n        )\n    def forward(self,x):\n        out = self.conv(x)\n        return out\n\nclass Discriminator(torch.nn.Module):\n    def __init__(self, in_channels=3,features=(64,128,256,512)):\n        super(Discriminator, self).__init__()\n        self.features = features\n        self.initial = torch.nn.Sequential(\n            torch.nn.Conv2d(in_channels=in_channels, out_channels=features[0], kernel_size=(4, 4),\n                            stride=(2,2), padding=1, bias=True, padding_mode='reflect'),\n            torch.nn.BatchNorm2d(num_features=features[0]),\n            torch.nn.LeakyReLU(negative_slope=0.2, inplace=True)\n        )\n        layers = []\n        in_channels=features[0]\n        for feature in features[1:]:\n            layers.append(\n                Block(in_channels,feature,stride=1 if feature == features[-1] else 2)\n            )\n            in_channels=feature\n        layers.append(torch.nn.Conv2d(in_channels=in_channels,out_channels=1,kernel_size=(4,4),\n                                      stride=(1,1),padding=1,padding_mode='reflect'))\n        #将值归一化到[0-1]\n        layers.append(torch.nn.Sigmoid())\n        #对layers进行解序列\n        self.model = torch.nn.Sequential(\n            *layers\n        )\n    def forward(self,x):\n        x = self.initial(x)\n        out= self.model(x)\n        return out\n\nif __name__ == '__main__':\n    x = torch.randn(size = (5,3,256,256),device='cpu')\n    model = Discriminator(in_channels=3)\n    preds = model(x)\n    print(preds.shape)","metadata":{"execution":{"iopub.status.busy":"2023-04-30T00:13:15.206758Z","iopub.execute_input":"2023-04-30T00:13:15.207330Z","iopub.status.idle":"2023-04-30T00:13:18.337550Z","shell.execute_reply.started":"2023-04-30T00:13:15.207293Z","shell.execute_reply":"2023-04-30T00:13:18.336347Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"torch.Size([5, 1, 30, 30])\n","output_type":"stream"}]},{"cell_type":"code","source":"\"\"\"\n@Author : Keep_Trying_Go\n@Major  : Computer Science and Technology\n@Hobby  : Computer Vision\n@Time   : 2023/4/28 17:14\n\"\"\"\n\nimport torch\nimport torchvision\nfrom torchinfo import summary\nfrom torchvision import transforms\n\nclass ConvBlock(torch.nn.Module):\n    def __init__(self,in_channels,out_channels,down=True,use_act=True,**kwargs):\n        super(ConvBlock, self).__init__()\n        self.conv = torch.nn.Sequential(\n            torch.nn.Conv2d(in_channels=in_channels,out_channels=out_channels,padding_mode='reflect',**kwargs)\n            if down\n            else torch.nn.ConvTranspose2d(in_channels=in_channels,out_channels=out_channels,**kwargs),\n            torch.nn.BatchNorm2d(num_features=out_channels),\n            torch.nn.ReLU(inplace=True) if use_act else torch.nn.Identity()\n        )\n    def forward(self,x):\n        return self.conv(x)\n\nclass ResidualBlock(torch.nn.Module):\n    def __init__(self,channels):\n        super(ResidualBlock, self).__init__()\n        self.block = torch.nn.Sequential(\n            ConvBlock(channels,channels,kernel_size = 3,padding = 1),\n            ConvBlock(channels,channels,use_act=False,kernel_size = 3,padding = 1),\n        )\n    def forward(self,x):\n        return x + self.block(x)\n\nclass Generator(torch.nn.Module):\n    def __init__(self,img_channels,num_features = 64,num_residual=9):\n        super(Generator, self).__init__()\n        self.initial = torch.nn.Sequential(\n            torch.nn.Conv2d(in_channels=img_channels,out_channels=num_features,kernel_size=(7,7),stride=(1,1),padding=3,padding_mode='reflect'),\n            torch.nn.ReLU(inplace=True)\n        )\n        self.down_blocks = torch.nn.ModuleList(\n            [\n                ConvBlock(in_channels=num_features,out_channels=num_features*2,kernel_size=(3,3),stride=(2,2),padding=1),\n                ConvBlock(in_channels=num_features*2, out_channels=num_features * 4, kernel_size=(3, 3), stride=(2, 2), padding=1)\n            ]\n        )\n        self.residual_blocks = torch.nn.Sequential(\n            *[ResidualBlock(num_features*4) for _ in range(num_residual)]\n        )\n        self.up_blocks = torch.nn.ModuleList(\n            [\n                ConvBlock(in_channels=num_features*4,out_channels=num_features*2,down=False,kernel_size=3,stride = 2,padding=1,output_padding=1),\n                ConvBlock(in_channels=num_features * 2, out_channels=num_features * 1, down=False, kernel_size=3, stride=2, padding=1,output_padding=1)\n            ]\n        )\n        self.last = torch.nn.Sequential(\n            torch.nn.Conv2d(in_channels=num_features, out_channels=img_channels, kernel_size=(7, 7), stride=(1, 1),\n                            padding=3, padding_mode='reflect'),\n            torch.nn.Tanh()\n        )\n    def forward(self,x):\n        x = self.initial(x)\n        for layer in self.down_blocks:\n            x = layer(x)\n        x = self.residual_blocks(x)\n        for layer in self.up_blocks:\n            x = layer(x)\n        out = self.last(x)\n        return out\n\nif __name__ == '__main__':\n    img_channels = 3\n    img_size = 256\n    x = torch.randn(size = (2,img_channels,img_size,img_size))\n    model = Generator(img_channels,9)\n    summary(model,input_size=(2,img_channels,img_size,img_size))\n#     img = model(x)\n#     print(img.shape)","metadata":{"execution":{"iopub.status.busy":"2023-04-30T00:13:20.495135Z","iopub.execute_input":"2023-04-30T00:13:20.495642Z","iopub.status.idle":"2023-04-30T00:13:25.742554Z","shell.execute_reply.started":"2023-04-30T00:13:20.495593Z","shell.execute_reply":"2023-04-30T00:13:25.741529Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"\"\"\"\n@Author : Keep_Trying_Go\n@Major  : Computer Science and Technology\n@Hobby  : Computer Vision\n@Time   : 2023/4/29 11:36\n\"\"\"\nimport torch\nimport albumentations #深度学习增强库\nfrom torchvision import transforms\nfrom albumentations.pytorch import ToTensorV2\n\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nTRAIN_DIR = \"/kaggle/input/cyclegandataset/vangogh2photo/train\"\nVAL_DIR = \"/kaggle/input/cyclegandataset/vangogh2photo/val\"\n\nBATCH_SIZE = 1\nLEARNING_RATE = 2e-4\nLAMBDA_IDENTITY = 0.0\nLAMBDA_CYCLE = 10\nNUM_WORKERS = 0\nNUM_EPOCHS = 20\nLOAD_MODEL = False\nSAVE_MODEL = True\nCHECKPOINT_GEN_H=\"gen.pth.tar\"\nCHECKPOINT_GEN_Z = \"genz.pth.tar\"\nCHECKPOINT_CRITICH_H=\"critich.pth.tar\"\nCHECKPOINT_CRITICH_Z=\"criticz.pth.tar\"\n\ntransform = albumentations.Compose(\n    [\n        albumentations.Resize(width=256,height=256),\n        albumentations.HorizontalFlip(p = 0.5),\n        albumentations.ColorJitter(p=0.1),#颜色抖动\n        albumentations.Normalize(mean=[0.5,0.5,0.5],std=[0.5,0.5,0.5]),\n        albumentations.pytorch.ToTensorV2()\n    ],\n    additional_targets={\"image0\":\"image\"},\n)\n","metadata":{"execution":{"iopub.status.busy":"2023-04-30T00:13:28.076089Z","iopub.execute_input":"2023-04-30T00:13:28.076863Z","iopub.status.idle":"2023-04-30T00:13:29.534034Z","shell.execute_reply.started":"2023-04-30T00:13:28.076823Z","shell.execute_reply":"2023-04-30T00:13:29.532989Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"\"\"\"\n@Author : Keep_Trying_Go\n@Major  : Computer Science and Technology\n@Hobby  : Computer Vision\n@Time   : 2023/4/29 11:27\n\"\"\"\n\nimport copy\nimport os\nimport random\n\nimport torch\nimport numpy as np\n\ndef save_checkpoint(model,optimizer,filename = \"my_checkpoint.pth.tar\",epochs = 0):\n    print(\"=> Saving checkpoint\")\n    checkpoint = {\n        \"state_dict\":model.state_dict(),\n        \"optimizer\":optimizer.state_dict(),\n    }\n    torch.save(checkpoint,filename + str(epochs))\ndef load_checkpoin(checkpoint_file,model,optimizer,lr):\n    print(\"=> Loading checkpoint\")\n    checkpoint = torch.load(checkpoint_file,map_location=DEVICE)\n    model.load_state_dict(checkpoint[\"state_dict\"])\n    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n\n    for param_group in optimizer.param_group:\n        param_group[\"lr\"] = lr\n\ndef seed_everthing(seed = 42):\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic=True\n    torch.backends.cudnn.benchmark=False\n\n\n\nif __name__ == '__main__':\n    pass\n","metadata":{"execution":{"iopub.status.busy":"2023-04-30T00:13:31.602244Z","iopub.execute_input":"2023-04-30T00:13:31.602639Z","iopub.status.idle":"2023-04-30T00:13:31.613308Z","shell.execute_reply.started":"2023-04-30T00:13:31.602591Z","shell.execute_reply":"2023-04-30T00:13:31.612222Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"\"\"\"\n@Author : Keep_Trying_Go\n@Major  : Computer Science and Technology\n@Hobby  : Computer Vision\n@Time   : 2023/4/28 17:14\n\"\"\"\n\nimport os\nimport torch\nimport numpy as np\nfrom PIL import Image\nfrom torchvision import transforms\nfrom torch.utils.data import DataLoader,Dataset\n\nclass vanGoPhotoDataset(Dataset):\n    def __init__(self,root_vango,root_photo,transform=None):\n        super(vanGoPhotoDataset, self).__init__()\n        self.root_vango = root_vango\n        self.root_photo = root_photo\n        self.transform = transform\n\n        self.vango_Images = os.listdir(self.root_vango)\n        self.photo_Images = os.listdir(self.root_photo)\n\n        self.length_dataset = max(len(self.vango_Images),len(self.photo_Images))\n        self.vango_len = len(self.vango_Images)\n        self.photo_len = len(self.photo_Images)\n\n    def __len__(self):\n        return self.length_dataset\n\n    def __getitem__(self, index):\n        vango_img = self.vango_Images[index % self.vango_len]\n        photo_img = self.photo_Images[index % self.photo_len]\n\n        vango_path = os.path.join(self.root_vango,vango_img)\n        photo_path = os.path.join(self.root_photo,photo_img)\n\n        vango_img = np.array(Image.open(vango_path).convert(\"RGB\"))\n        photo_img = np.array(Image.open(photo_path).convert(\"RGB\"))\n\n        if self.transform:\n            argumentation = self.transform(image = vango_img,image0 = photo_img)\n            vango_img = argumentation[\"image\"]\n            photo_img = argumentation[\"image0\"]\n        return vango_img,photo_img\n\n\n","metadata":{"execution":{"iopub.status.busy":"2023-04-30T00:13:36.016360Z","iopub.execute_input":"2023-04-30T00:13:36.016867Z","iopub.status.idle":"2023-04-30T00:13:36.034799Z","shell.execute_reply.started":"2023-04-30T00:13:36.016823Z","shell.execute_reply":"2023-04-30T00:13:36.033711Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"\"\"\"\n@Author : Keep_Trying_Go\n@Major  : Computer Science and Technology\n@Hobby  : Computer Vision\n@Time   : 2023/4/28 17:14\n\"\"\"\nimport os\nimport sys\nimport torch\nimport numpy as np\nfrom PIL import Image\nfrom tqdm import tqdm\nfrom torchvision import transforms\n\nfrom torchvision.utils import save_image\nfrom torch.utils.data import DataLoader,Dataset\n\ndef train_fn(disc_H,disc_Z,gen_H,gen_Z,loader,opt_disc,opt_gen,L1,mse,d_scale,g_scale,epoch):\n    loop = tqdm(loader,leave=True)\n    for idx ,(vango,photo) in enumerate(loop):\n        vango = vango.to(DEVICE)\n        photo = photo.to(DEVICE)\n        \n        #train discriminator\n        with torch.cuda.amp.autocast():\n            fake_photo = gen_H(vango)\n            D_H_real = disc_H(photo)\n            D_H_fake = disc_H(fake_photo.detach())\n            D_H_real_loss = mse(D_H_real,torch.ones_like(D_H_real))\n            D_H_fake_loss = mse(D_H_fake, torch.zeros_like(D_H_fake))\n            D_H_loss = D_H_fake_loss + D_H_real_loss\n\n            fake_vango = gen_Z(photo)\n            D_Z_real = disc_Z(vango)\n            D_Z_fake = disc_Z(fake_vango.detach())\n            D_Z_real_loss = mse(D_Z_real, torch.ones_like(D_Z_real))\n            D_Z_fake_loss = mse(D_Z_fake, torch.zeros_like(D_Z_fake))\n            D_Z_loss = D_Z_fake_loss + D_Z_real_loss\n\n            D_loss = D_H_loss + D_Z_loss\n\n        opt_disc.zero_grad()\n        d_scale.scale(D_loss).backward()\n        d_scale.step(opt_disc)\n        d_scale.update()\n\n    #train Generator H and Z\n        with torch.cuda.amp.autocast():\n            #adversarial loss for both generator\n            D_H_fake = disc_H(fake_photo)\n            D_Z_fake = disc_Z(fake_vango)\n            loss_G_Z = mse(D_H_fake,torch.ones_like(D_H_fake))\n            loss_G_H = mse(D_H_fake,torch.ones_like(D_Z_fake))\n\n            #cycle loss\n            cycle_vango = gen_Z(fake_photo)\n            cycle_photo = gen_H(fake_vango)\n            cycle_vango_loss = L1(vango,cycle_vango)\n            cycle_photo_loss = L1(photo,cycle_photo)\n\n            # identity loss\n            identity_vango = gen_Z(vango)\n            identity_photo = gen_H(photo)\n            identity_vango_loss = L1(vango,identity_vango)\n            identity_photo_loss = L1(photo,identity_photo)\n\n            G_loss = (\n                loss_G_H + loss_G_Z\n                + cycle_vango_loss * LAMBDA_CYCLE\n                +cycle_photo_loss * LAMBDA_CYCLE\n                +identity_vango_loss * LAMBDA_IDENTITY\n                +identity_photo_loss * LAMBDA_IDENTITY\n            )\n        opt_gen.zero_grad()\n        g_scale.scale(G_loss).backward()\n        g_scale.step(opt_gen)\n        g_scale.update()\n\n\n        with torch.no_grad():\n            if idx % 1000 == 0:\n                save_image(fake_photo*0.5 + 0.5,f\"VangoTophoto{epoch}_{idx}.png\")\n                save_image(fake_vango*0.5 + 0.5,f\"phtotToVango{epoch}_{idx}.png\")\n                print(\"******************************************************************\\n\")\n                print(\"--------------------G_loss : {:.6}-------------------\".format(G_loss))\n                print(\"--------------------D_loss : {:.6}-------------------\".format(D_loss))\n\n\ndef main_():\n    disc_H = Discriminator(in_channels=3).to(DEVICE)\n    disc_Z = Discriminator(in_channels=3).to(DEVICE)\n    gen_Z = Generator(img_channels=3,num_residual=9).to(DEVICE)\n    gen_H = Generator(img_channels=3,num_residual=9).to(DEVICE)\n    opt_disc = torch.optim.Adam(\n        list(disc_H.parameters()) + list(disc_Z.parameters()),\n        lr = LEARNING_RATE,\n        betas=(0.5,0.999)\n    )\n    opt_gen = torch.optim.Adam(\n        list(gen_H.parameters()) + list(gen_Z.parameters()),\n        lr = LEARNING_RATE,\n        betas=(0.5,0.999)\n    )\n    L1 = torch.nn.L1Loss()\n    mse = torch.nn.MSELoss()\n\n    #导入预训练模型\n    if LOAD_MODEL:\n        load_checkpoin(\n            CHECKPOINT_GEN_H,gen_H,opt_gen,LEARNING_RATE\n        )\n        load_checkpoin(\n            CHECKPOINT_GEN_Z, gen_Z, opt_gen, LEARNING_RATE\n        )\n        load_checkpoin(\n            CHECKPOINT_CRITICH_H, disc_H, opt_disc, LEARNING_RATE\n        )\n        load_checkpoin(\n            CHECKPOINT_CRITICH_Z, disc_Z, opt_disc, LEARNING_RATE\n        )\n\n    dataset = vanGoPhotoDataset(\n        root_vango=TRAIN_DIR + \"/trainA\",root_photo=TRAIN_DIR + \"/trainB\",\n        transform=transform\n    )\n\n    loader = DataLoader(\n        dataset=dataset,\n        batch_size=BATCH_SIZE,\n        shuffle=True,\n        num_workers=NUM_WORKERS,\n        pin_memory=False\n    )\n    g_scale = torch.cuda.amp.GradScaler()\n    d_scale = torch.cuda.amp.GradScaler()\n\n    for epoch in range(NUM_EPOCHS):\n        train_fn(disc_H,disc_Z,gen_H,gen_Z,loader,opt_disc,opt_gen,L1,mse,d_scale,g_scale,epoch)\n        if SAVE_MODEL:\n            save_checkpoint(gen_H,opt_gen,filename=CHECKPOINT_GEN_H,epochs = epoch)\n            save_checkpoint(gen_Z, opt_gen, filename=CHECKPOINT_GEN_Z,epochs = epoch)\n            save_checkpoint(disc_H, opt_disc, filename=CHECKPOINT_CRITICH_H,epochs = epoch)\n            save_checkpoint(disc_Z, opt_disc, filename=CHECKPOINT_CRITICH_Z,epochs = epoch)\n\n\nif __name__ == '__main__':\n    main_()\n","metadata":{"execution":{"iopub.status.busy":"2023-04-30T00:13:44.321321Z","iopub.execute_input":"2023-04-30T00:13:44.321862Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"  0%|          | 1/6287 [00:01<2:18:45,  1.32s/it]","output_type":"stream"},{"name":"stdout","text":"******************************************************************\n\n--------------------G_loss : 10.689-------------------\n--------------------D_loss : 1.03261-------------------\n","output_type":"stream"},{"name":"stderr","text":" 16%|█▌        | 1001/6287 [10:03<53:54,  1.63it/s] ","output_type":"stream"},{"name":"stdout","text":"******************************************************************\n\n--------------------G_loss : 5.08701-------------------\n--------------------D_loss : 0.519297-------------------\n","output_type":"stream"},{"name":"stderr","text":" 32%|███▏      | 2001/6287 [20:04<43:29,  1.64it/s]","output_type":"stream"},{"name":"stdout","text":"******************************************************************\n\n--------------------G_loss : 4.16226-------------------\n--------------------D_loss : 0.574354-------------------\n","output_type":"stream"},{"name":"stderr","text":" 48%|████▊     | 3001/6287 [30:05<33:29,  1.64it/s]","output_type":"stream"},{"name":"stdout","text":"******************************************************************\n\n--------------------G_loss : 5.24547-------------------\n--------------------D_loss : 0.542539-------------------\n","output_type":"stream"},{"name":"stderr","text":" 64%|██████▎   | 4001/6287 [40:06<23:27,  1.62it/s]","output_type":"stream"},{"name":"stdout","text":"******************************************************************\n\n--------------------G_loss : 4.1881-------------------\n--------------------D_loss : 0.542041-------------------\n","output_type":"stream"},{"name":"stderr","text":" 80%|███████▉  | 5001/6287 [50:08<13:07,  1.63it/s]","output_type":"stream"},{"name":"stdout","text":"******************************************************************\n\n--------------------G_loss : 3.9214-------------------\n--------------------D_loss : 0.983858-------------------\n","output_type":"stream"},{"name":"stderr","text":" 95%|█████████▌| 6001/6287 [1:00:11<02:55,  1.63it/s]","output_type":"stream"},{"name":"stdout","text":"******************************************************************\n\n--------------------G_loss : 3.8233-------------------\n--------------------D_loss : 0.565725-------------------\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 6287/6287 [1:03:04<00:00,  1.66it/s]\n","output_type":"stream"},{"name":"stdout","text":"=> Saving checkpoint\n=> Saving checkpoint\n=> Saving checkpoint\n=> Saving checkpoint\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 1/6287 [00:00<1:06:37,  1.57it/s]","output_type":"stream"},{"name":"stdout","text":"******************************************************************\n\n--------------------G_loss : 4.02927-------------------\n--------------------D_loss : 0.335928-------------------\n","output_type":"stream"},{"name":"stderr","text":" 16%|█▌        | 1001/6287 [10:00<54:02,  1.63it/s] ","output_type":"stream"},{"name":"stdout","text":"******************************************************************\n\n--------------------G_loss : 3.92392-------------------\n--------------------D_loss : 0.28532-------------------\n","output_type":"stream"},{"name":"stderr","text":" 32%|███▏      | 2001/6287 [19:59<43:49,  1.63it/s]","output_type":"stream"},{"name":"stdout","text":"******************************************************************\n\n--------------------G_loss : 3.64139-------------------\n--------------------D_loss : 0.616068-------------------\n","output_type":"stream"},{"name":"stderr","text":" 48%|████▊     | 3001/6287 [29:59<33:19,  1.64it/s]","output_type":"stream"},{"name":"stdout","text":"******************************************************************\n\n--------------------G_loss : 3.28053-------------------\n--------------------D_loss : 0.457228-------------------\n","output_type":"stream"},{"name":"stderr","text":" 64%|██████▎   | 4001/6287 [39:58<23:06,  1.65it/s]","output_type":"stream"},{"name":"stdout","text":"******************************************************************\n\n--------------------G_loss : 3.3704-------------------\n--------------------D_loss : 0.517752-------------------\n","output_type":"stream"},{"name":"stderr","text":" 80%|███████▉  | 5001/6287 [49:56<13:02,  1.64it/s]","output_type":"stream"},{"name":"stdout","text":"******************************************************************\n\n--------------------G_loss : 3.269-------------------\n--------------------D_loss : 0.516287-------------------\n","output_type":"stream"},{"name":"stderr","text":" 95%|█████████▌| 6001/6287 [59:54<02:54,  1.63it/s]","output_type":"stream"},{"name":"stdout","text":"******************************************************************\n\n--------------------G_loss : 3.66621-------------------\n--------------------D_loss : 0.587637-------------------\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 6287/6287 [1:02:45<00:00,  1.67it/s]\n","output_type":"stream"},{"name":"stdout","text":"=> Saving checkpoint\n=> Saving checkpoint\n=> Saving checkpoint\n=> Saving checkpoint\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 1/6287 [00:00<1:05:54,  1.59it/s]","output_type":"stream"},{"name":"stdout","text":"******************************************************************\n\n--------------------G_loss : 3.09499-------------------\n--------------------D_loss : 0.304768-------------------\n","output_type":"stream"},{"name":"stderr","text":" 16%|█▌        | 1001/6287 [09:57<53:23,  1.65it/s] ","output_type":"stream"},{"name":"stdout","text":"******************************************************************\n\n--------------------G_loss : 3.15497-------------------\n--------------------D_loss : 0.446501-------------------\n","output_type":"stream"},{"name":"stderr","text":" 32%|███▏      | 2001/6287 [19:54<43:31,  1.64it/s]","output_type":"stream"},{"name":"stdout","text":"******************************************************************\n\n--------------------G_loss : 3.14788-------------------\n--------------------D_loss : 0.694607-------------------\n","output_type":"stream"},{"name":"stderr","text":" 48%|████▊     | 3001/6287 [29:52<33:16,  1.65it/s]","output_type":"stream"},{"name":"stdout","text":"******************************************************************\n\n--------------------G_loss : 2.73008-------------------\n--------------------D_loss : 0.470462-------------------\n","output_type":"stream"},{"name":"stderr","text":" 64%|██████▎   | 4001/6287 [39:50<23:07,  1.65it/s]","output_type":"stream"},{"name":"stdout","text":"******************************************************************\n\n--------------------G_loss : 3.28358-------------------\n--------------------D_loss : 0.495972-------------------\n","output_type":"stream"},{"name":"stderr","text":" 80%|███████▉  | 5001/6287 [49:46<13:00,  1.65it/s]","output_type":"stream"},{"name":"stdout","text":"******************************************************************\n\n--------------------G_loss : 2.98879-------------------\n--------------------D_loss : 0.530085-------------------\n","output_type":"stream"},{"name":"stderr","text":" 95%|█████████▌| 6001/6287 [59:43<02:54,  1.64it/s]","output_type":"stream"},{"name":"stdout","text":"******************************************************************\n\n--------------------G_loss : 3.22349-------------------\n--------------------D_loss : 0.52813-------------------\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 6287/6287 [1:02:34<00:00,  1.67it/s]\n","output_type":"stream"},{"name":"stdout","text":"=> Saving checkpoint\n=> Saving checkpoint\n=> Saving checkpoint\n=> Saving checkpoint\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 1/6287 [00:00<1:06:22,  1.58it/s]","output_type":"stream"},{"name":"stdout","text":"******************************************************************\n\n--------------------G_loss : 3.01564-------------------\n--------------------D_loss : 0.511454-------------------\n","output_type":"stream"},{"name":"stderr","text":" 16%|█▌        | 1001/6287 [09:56<53:31,  1.65it/s] ","output_type":"stream"},{"name":"stdout","text":"******************************************************************\n\n--------------------G_loss : 3.02639-------------------\n--------------------D_loss : 0.226997-------------------\n","output_type":"stream"},{"name":"stderr","text":" 32%|███▏      | 2001/6287 [19:53<43:28,  1.64it/s]","output_type":"stream"},{"name":"stdout","text":"******************************************************************\n\n--------------------G_loss : 3.07585-------------------\n--------------------D_loss : 0.298132-------------------\n","output_type":"stream"},{"name":"stderr","text":" 48%|████▊     | 3001/6287 [29:49<33:08,  1.65it/s]","output_type":"stream"},{"name":"stdout","text":"******************************************************************\n\n--------------------G_loss : 3.00439-------------------\n--------------------D_loss : 0.502537-------------------\n","output_type":"stream"},{"name":"stderr","text":" 64%|██████▎   | 4001/6287 [39:45<23:02,  1.65it/s]","output_type":"stream"},{"name":"stdout","text":"******************************************************************\n\n--------------------G_loss : 2.89359-------------------\n--------------------D_loss : 0.658576-------------------\n","output_type":"stream"},{"name":"stderr","text":" 80%|███████▉  | 5001/6287 [49:41<12:59,  1.65it/s]","output_type":"stream"},{"name":"stdout","text":"******************************************************************\n\n--------------------G_loss : 2.7869-------------------\n--------------------D_loss : 0.472824-------------------\n","output_type":"stream"},{"name":"stderr","text":" 92%|█████████▏| 5758/6287 [57:13<05:14,  1.68it/s]","output_type":"stream"}]},{"cell_type":"code","source":"# import shutil\n# import os\n \n# if __name__ == '__main__':\n#     path = '/kaggle/working'\n#     if os.path.exists(path):\n#         shutil.rmtree(path)\n#         print('删除完成')\n#     else:\n#         print('原本为空')","metadata":{"execution":{"iopub.status.busy":"2023-04-30T00:13:05.986554Z","iopub.execute_input":"2023-04-30T00:13:05.987225Z","iopub.status.idle":"2023-04-30T00:13:05.991959Z","shell.execute_reply.started":"2023-04-30T00:13:05.987189Z","shell.execute_reply":"2023-04-30T00:13:05.990953Z"},"trusted":true},"execution_count":2,"outputs":[]}]}